#!/usr/bin/env python3
"""Agentic AI æ™‚ç³»åˆ—äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹è‰¯ç‰ˆãƒ»ä¿®æ­£æ¸ˆã¿ï¼‰

ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆ258è¡Œï¼‰ã«æœ€é©åŒ–ã—ãŸMOMENTã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

äºˆæ¸¬å¯¾è±¡: 'agentic artificial intelligence_semantic-count'åˆ—ã®å°†æ¥å€¤
å­¦ç¿’å¯¾è±¡: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿é‡ã«åˆã‚ã›ãŸçŸ­ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
äºˆæ¸¬æœŸé–“: ç¾å®Ÿçš„ãªç¯„å›²ã§ã®äºˆæ¸¬è©•ä¾¡
"""

import warnings
from pathlib import Path

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.cuda.amp

# MOMENTé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from momentfm import MOMENTPipeline
from momentfm.utils.utils import control_randomness
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm


warnings.filterwarnings("ignore")

# è¨­å®šï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
RANDOM_SEED = 42
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
FORECAST_HORIZON = 12  # 12ãƒ¶æœˆäºˆæ¸¬

print(f"ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
print(f"ğŸ”§ äºˆæ¸¬æœŸé–“: {FORECAST_HORIZON}")


def determine_context_length(data_length: int, forecast_horizon: int) -> int:
    """ãƒ‡ãƒ¼ã‚¿é‡ã«åŸºã¥ã„ã¦é©åˆ‡ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ±ºå®š"""
    available = data_length - forecast_horizon

    # MOMENTãŒåŠ¹ç‡çš„ã«å‹•ä½œã™ã‚‹é•·ã•ã®å€™è£œ
    candidates = [48, 96, 192, 256, 384, 512]

    best_length = 48  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    for length in candidates:
        if available >= length + 20:  # ä½™è£•ã‚’æŒã£ã¦20ä»¥ä¸Š
            best_length = length
        else:
            break

    # æœ€ä½é™å¿…è¦ãªé•·ã•ã‚’ç¢ºä¿
    best_length = max(best_length, 24)  # æœ€ä½2å¹´åˆ†
    best_length = min(best_length, available - 10)  # ä¸Šé™è¨­å®š

    return best_length


class TimeSeriesDataset(Dataset):
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·å¯¾å¿œï¼‰"""

    def __init__(
        self,
        data: np.ndarray,
        context_length: int,
        forecast_horizon: int = 12,
    ):
        # å˜å¤‰é‡ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
        if data.ndim > 1:
            self.data = data[:, 0]  # æœ€åˆã®åˆ—ï¼ˆagentic AIé–¢é€£ï¼‰ã®ã¿ä½¿ç”¨
        else:
            self.data = data

        self.context_length = context_length
        self.forecast_horizon = forecast_horizon

        # ãƒ‡ãƒ¼ã‚¿é•·ã®æ¤œè¨¼
        min_required = context_length + forecast_horizon
        if len(self.data) < min_required:
            raise ValueError(
                f"ãƒ‡ãƒ¼ã‚¿é•·ãŒä¸è¶³: å¿…è¦={min_required}, å®Ÿéš›={len(self.data)}",
            )

        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†:")
        print(f"   - ãƒ‡ãƒ¼ã‚¿é•·: {len(self.data)}")
        print(f"   - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {context_length}")
        print(f"   - äºˆæ¸¬æœŸé–“: {forecast_horizon}")

    def __len__(self) -> int:
        return max(1, len(self.data) - self.context_length - self.forecast_horizon + 1)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        input_data = self.data[idx : idx + self.context_length]
        # äºˆæ¸¬å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        target_data = self.data[
            idx + self.context_length : idx
            + self.context_length
            + self.forecast_horizon
        ]

        # MOMENTã®æœŸå¾…å…¥åŠ›é•·: 512
        moment_input_length = 512

        # 512ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        padded_input = np.zeros(moment_input_length)
        if self.context_length <= moment_input_length:
            # æœ«å°¾ã«ãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’é‡è¦–ï¼‰
            start_idx = moment_input_length - self.context_length
            padded_input[start_idx:] = input_data
            # input_maskä½œæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã¯1ã€paddingéƒ¨åˆ†ã¯0ï¼‰
            input_mask = np.zeros(moment_input_length)
            input_mask[start_idx:] = 1.0
        else:
            # ãƒ‡ãƒ¼ã‚¿ãŒ512ã‚ˆã‚Šé•·ã„å ´åˆã¯æœ«å°¾512ç‚¹ã‚’ä½¿ç”¨
            padded_input = input_data[-moment_input_length:]
            input_mask = np.ones(moment_input_length)

        # MOMENTã®æœŸå¾…å½¢å¼: [channels, sequence_length]
        input_tensor = torch.tensor(padded_input, dtype=torch.float32).unsqueeze(
            0
        )  # [1, 512]
        target_tensor = torch.tensor(target_data, dtype=torch.float32).unsqueeze(
            0
        )  # [1, forecast_horizon]
        mask_tensor = torch.tensor(input_mask, dtype=torch.float32)  # [512]

        return input_tensor, target_tensor, mask_tensor


class AgenticAIForecaster:
    """Agentic AI æ™‚ç³»åˆ—äºˆæ¸¬ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆãƒ»ä¿®æ­£æ¸ˆã¿ï¼‰"""

    def __init__(self, data_path: str, output_dir: str = "results"):
        self.data_path = Path(data_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        self.df: pd.DataFrame | None = None
        self.model: MOMENTPipeline | None = None
        self.results: dict = {}
        self.context_length: int | None = None

        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")

    def load_and_analyze_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€é©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®š"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»åˆ†æä¸­...")

        try:
            self.df = pd.read_excel(self.data_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.df.shape}")

            # timelineåˆ—ã‚’datetimeã«å¤‰æ›
            if "timeline" in self.df.columns:
                self.df["timeline"] = pd.to_datetime(self.df["timeline"])
                self.df = self.df.sort_values("timeline").reset_index(drop=True)
                print(
                    f"ğŸ“… æ—¥ä»˜ç¯„å›²: {self.df['timeline'].min()} ï½ {self.df['timeline'].max()}",
                )

            # é©åˆ‡ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ±ºå®š
            data_length = len(self.df)
            self.context_length = determine_context_length(
                data_length,
                FORECAST_HORIZON,
            )

            print("ğŸ¯ æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            print(f"   - ãƒ‡ãƒ¼ã‚¿ç·æ•°: {data_length}")
            print(f"   - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {self.context_length}")
            print(f"   - äºˆæ¸¬æœŸé–“: {FORECAST_HORIZON}")
            print(
                f"   - å­¦ç¿’å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒ«æ•°: {data_length - self.context_length - FORECAST_HORIZON + 1}",
            )

            return self.df

        except Exception as e:
            raise Exception(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def prepare_data(
        self,
        end_date: str = "2025-06-01",
        target_column: str = "agentic artificial intelligence_semantic-count",
    ) -> tuple[np.ndarray, pd.DatetimeIndex]:
        """äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        if self.df is None:
            raise ValueError("å…ˆã«load_and_analyze_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

        # æŒ‡å®šæ—¥ä»¥å‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        end_datetime = pd.to_datetime(end_date)
        mask = self.df["timeline"] <= end_datetime
        filtered_df = self.df[mask].copy()

        if len(filtered_df) == 0:
            raise ValueError(f"æŒ‡å®šæ—¥ {end_date} ä»¥å‰ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        # å¯¾è±¡åˆ—ã®ç¢ºèªãƒ»é¸æŠ
        if target_column not in filtered_df.columns:
            numeric_cols = [
                col
                for col in filtered_df.columns
                if col != "timeline" and pd.api.types.is_numeric_dtype(filtered_df[col])
            ]
            target_column = numeric_cols[0]
            print(f"ğŸ”„ ä»£æ›¿åˆ—ã‚’ä½¿ç”¨: {target_column}")

        # å˜å¤‰é‡ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡º
        data = filtered_df[target_column].values
        dates = filtered_df["timeline"].values

        # æ¬ æå€¤ã®å‡¦ç†ï¼ˆæ–°ã—ã„pandasè¨˜æ³•ã‚’ä½¿ç”¨ï¼‰
        if pd.isna(data).any():
            print("âš ï¸ æ¬ æå€¤å‡¦ç†ä¸­...")
            data = pd.Series(data).ffill().bfill().values

        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {data.shape} (çµ‚äº†: {end_date})")
        print(
            f"ğŸ“Š çµ±è¨ˆ: min={np.min(data):.1f}, max={np.max(data):.1f}, mean={np.mean(data):.1f}",
        )

        return data.reshape(-1, 1), pd.DatetimeIndex(dates)

    def initialize_model(self, forecast_horizon: int = 12) -> MOMENTPipeline:
        """MOMENTãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆ512å…¥åŠ›é•·å¯¾å¿œï¼‰"""
        print("ğŸ¤– MOMENTãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")

        model = MOMENTPipeline.from_pretrained(
            "AutonLab/MOMENT-1-large",
            model_kwargs={
                "task_name": "forecasting",
                "forecast_horizon": forecast_horizon,
                "head_dropout": 0.1,
                "weight_decay": 0,
                "freeze_encoder": True,
                "freeze_embedder": True,
                "freeze_head": False,
            },
        )

        model.init()
        model = model.to(DEVICE)

        print("âœ… MOMENTãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†ï¼ˆå…¥åŠ›é•·: 512ï¼‰")
        print("ğŸ’¡ çŸ­ã„ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«512ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã¾ã™")
        return model

    def train_model(
        self,
        train_data: np.ndarray,
        epochs: int = 3,
        batch_size: int = 4,
        lr: float = 1e-4,
    ) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        if self.context_length is None:
            raise ValueError("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹ (epochs: {epochs})")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = TimeSeriesDataset(train_data, self.context_length, FORECAST_HORIZON)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        if len(dataset) == 0:
            raise ValueError("è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã§ã™")

        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")

        # è¨“ç·´è¨­å®š
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.Adam(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=lr,
        )
        scaler = torch.cuda.amp.GradScaler() if DEVICE.type == "cuda" else None

        self.model.train()

        for epoch in range(epochs):
            epoch_losses = []
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{epochs}")

            for batch_input, batch_target, batch_mask in progress_bar:
                # MOMENTã®æœŸå¾…ã™ã‚‹å½¢çŠ¶: [batch_size, n_channels, 512]
                batch_input = batch_input.to(DEVICE)  # [batch_size, 1, 512]
                batch_target = batch_target.to(DEVICE)  # [batch_size, 1, forecast_len]
                batch_mask = batch_mask.to(DEVICE)  # [batch_size, 512]

                optimizer.zero_grad()

                try:
                    if scaler:
                        with torch.cuda.amp.autocast():
                            output = self.model(
                                x_enc=batch_input, input_mask=batch_mask
                            )
                            loss = criterion(output.forecast, batch_target)

                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        output = self.model(x_enc=batch_input, input_mask=batch_mask)
                        loss = criterion(output.forecast, batch_target)
                        loss.backward()
                        optimizer.step()

                    epoch_losses.append(loss.item())
                    progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

                except Exception as e:
                    print(f"âŒ ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")
                    print(f"å…¥åŠ›å½¢çŠ¶: {batch_input.shape}")
                    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå½¢çŠ¶: {batch_target.shape}")
                    print(f"ãƒã‚¹ã‚¯å½¢çŠ¶: {batch_mask.shape}")
                    print(f"å…¥åŠ›å½¢çŠ¶: {batch_input.shape}")
                    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå½¢çŠ¶: {batch_target.shape}")
                    raise

            avg_loss = np.mean(epoch_losses) if epoch_losses else 0
            print(f"Epoch {epoch + 1} å®Œäº† - å¹³å‡æå¤±: {avg_loss:.4f}")

        print("âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

    def predict(self, input_data: np.ndarray) -> np.ndarray:
        """äºˆæ¸¬å®Ÿè¡Œ"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        if self.context_length is None:
            raise ValueError("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        self.model.eval()

        with torch.no_grad():
            # å˜å¤‰é‡ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†
            if input_data.ndim > 1:
                input_data = input_data[:, 0]

            # æœ€å¾Œã®context_lengthåˆ†ã‚’å–å¾—
            if len(input_data) >= self.context_length:
                input_sequence = input_data[-self.context_length :]
            else:
                input_sequence = input_data
                print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿é•·ãŒä¸è¶³: {len(input_data)} < {self.context_length}")

            # MOMENTã®æœŸå¾…å…¥åŠ›é•·: 512
            moment_input_length = 512

            # 512ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padded_input = np.zeros(moment_input_length)
            if len(input_sequence) <= moment_input_length:
                # æœ«å°¾ã«ãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’é‡è¦–ï¼‰
                start_idx = moment_input_length - len(input_sequence)
                padded_input[start_idx:] = input_sequence
                # input_maskä½œæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã¯1ã€paddingéƒ¨åˆ†ã¯0ï¼‰
                input_mask = np.zeros(moment_input_length)
                input_mask[start_idx:] = 1.0
            else:
                # ãƒ‡ãƒ¼ã‚¿ãŒ512ã‚ˆã‚Šé•·ã„å ´åˆã¯æœ«å°¾512ç‚¹ã‚’ä½¿ç”¨
                padded_input = input_sequence[-moment_input_length:]
                input_mask = np.ones(moment_input_length)

            # [512] â†’ [1, 1, 512]
            input_tensor = (
                torch.tensor(padded_input, dtype=torch.float32)
                .unsqueeze(0)  # ãƒãƒƒãƒæ¬¡å…ƒ
                .unsqueeze(0)  # ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒ
                .to(DEVICE)
            )

            # input_mask [512] â†’ [1, 512] (ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ )
            mask_tensor = (
                torch.tensor(input_mask, dtype=torch.float32).unsqueeze(0).to(DEVICE)
            )

            try:
                output = self.model(x_enc=input_tensor, input_mask=mask_tensor)
                if hasattr(output.forecast, "cpu"):
                    prediction = output.forecast.cpu().numpy().squeeze()
                else:
                    prediction = output.forecast.squeeze()
                return prediction

            except Exception as e:
                print(f"âŒ äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"å…¥åŠ›å½¢çŠ¶: {input_tensor.shape}")
                print(f"ãƒã‚¹ã‚¯å½¢çŠ¶: {mask_tensor.shape}")
                raise

    def run_evaluation(self, max_years_back: int = 3) -> dict:
        """è©•ä¾¡å®Ÿè¡Œï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦èª¿æ•´ï¼‰"""
        if self.df is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        if self.context_length is None:
            raise ValueError("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿é‡ã«åŸºã¥ã„ã¦ç¾å®Ÿçš„ãªè©•ä¾¡ç¯„å›²ã‚’æ±ºå®š
        total_months = len(self.df)
        max_possible_years = (
            total_months - self.context_length - FORECAST_HORIZON
        ) // 12

        years_back_list = list(
            range(1, min(max_years_back + 1, max_possible_years + 1)),
        )

        if not years_back_list:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return {}

        print(f"ğŸ” è©•ä¾¡é–‹å§‹: {len(years_back_list)}å¹´åˆ†ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ")
        print(f"   è©•ä¾¡ç¯„å›²: {years_back_list}")

        results = {}

        for years_back in years_back_list:
            print(f"\nğŸ“… {years_back}å¹´å‰ã‹ã‚‰ã®äºˆæ¸¬...")

            # è¨“ç·´çµ‚äº†æ—¥ã®è¨­å®š
            train_end_date = pd.to_datetime("2025-06-01") - pd.DateOffset(
                years=years_back,
            )
            train_end_str = train_end_date.strftime("%Y-%m-01")

            try:
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                train_data, train_dates = self.prepare_data(train_end_str)

                # ãƒ‡ãƒ¼ã‚¿é•·ãƒã‚§ãƒƒã‚¯
                required_length = self.context_length + FORECAST_HORIZON
                if len(train_data) < required_length:
                    print(
                        f"âš ï¸ ãƒ‡ãƒ¼ã‚¿é•·ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {len(train_data)} < {required_length}",
                    )
                    continue

                # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã¨è¨“ç·´
                self.model = self.initialize_model(FORECAST_HORIZON)
                self.train_model(train_data, epochs=2)

                # äºˆæ¸¬å®Ÿè¡Œ
                prediction = self.predict(train_data)

                # äºˆæ¸¬æœŸé–“ã®æ—¥ä»˜ç”Ÿæˆ
                last_date = train_dates[-1]
                forecast_dates = pd.date_range(
                    start=last_date + pd.DateOffset(months=1),
                    periods=FORECAST_HORIZON,
                    freq="MS",
                )

                results[years_back] = {
                    "train_end_date": train_end_str,
                    "prediction": prediction,
                    "train_dates": train_dates,
                    "forecast_dates": forecast_dates,
                    "train_data": train_data,
                }

                print(f"âœ… {years_back}å¹´å‰ã‹ã‚‰ã®äºˆæ¸¬å®Œäº†")
                print(
                    f"ğŸ“ˆ äºˆæ¸¬å€¤ç¯„å›²: {np.min(prediction):.1f} ï½ {np.max(prediction):.1f}",
                )

            except Exception as e:
                print(f"âŒ {years_back}å¹´å‰ã®äºˆæ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        self.results = results
        print(f"\nğŸ¯ è©•ä¾¡å®Œäº†: {len(results)}/{len(years_back_list)} ã®äºˆæ¸¬ãŒæˆåŠŸ")
        return results

    def get_actual_values(
        self, start_date: pd.Timestamp, periods: int
    ) -> tuple[np.ndarray, pd.DatetimeIndex]:
        """å®Ÿæ¸¬å€¤ã‚’å–å¾—ã™ã‚‹"""
        try:
            # å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿæ¸¬å€¤ã‚’å–å¾—
            if self.df is None:
                return np.array([]), pd.DatetimeIndex([])

            # å¯¾è±¡åˆ—ï¼ˆagentic AIé–¢é€£ã®æ•°å€¤åˆ—ï¼‰ã‚’å–å¾—
            target_col = None
            for col in self.df.columns[1:]:  # æœ€åˆã®åˆ—ã¯æ—¥ä»˜ã¨ä»®å®š
                if pd.api.types.is_numeric_dtype(self.df[col]):
                    target_col = col
                    break

            if target_col is None:
                return np.array([]), pd.DatetimeIndex([])

            # æ—¥ä»˜åˆ—ã‚’å–å¾—
            date_col = self.df.columns[0]
            df_copy = self.df.copy()
            df_copy[date_col] = pd.to_datetime(df_copy[date_col])

            # æŒ‡å®šæœŸé–“ã®å®Ÿæ¸¬å€¤ã‚’æŠ½å‡º
            end_date = start_date + pd.DateOffset(months=periods - 1)
            mask = (df_copy[date_col] >= start_date) & (df_copy[date_col] <= end_date)
            actual_data = df_copy.loc[mask, target_col].values
            actual_dates = pd.date_range(
                start=start_date, periods=len(actual_data), freq="MS"
            )

            return actual_data, actual_dates

        except Exception as e:
            print(f"âš ï¸ å®Ÿæ¸¬å€¤å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return np.array([]), pd.DatetimeIndex([])

    def visualize_results(self, save_plots: bool = True) -> None:
        """çµæœã®å¯è¦–åŒ–ï¼ˆæ¨ªé•·ãƒ—ãƒ­ãƒƒãƒˆï¼‰"""
        if not self.results:
            print("âŒ çµæœãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        print("ğŸ“Š çµæœå¯è¦–åŒ–ä¸­...")

        # æ¨ªé•·ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
        fig, axes = plt.subplots(
            len(self.results),
            1,
            figsize=(24, 6 * len(self.results)),
        )

        if len(self.results) == 1:
            axes = [axes]

        for idx, (years_back, result) in enumerate(self.results.items()):
            ax = axes[idx]

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
            train_data = result["train_data"][:, 0]
            train_dates = result["train_dates"]

            ax.plot(
                train_dates,
                train_data,
                color="steelblue",
                linewidth=2,
                label="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿",
                alpha=0.8,
            )

            # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
            prediction = result["prediction"]
            forecast_dates = result["forecast_dates"]

            ax.plot(
                forecast_dates,
                prediction,
                color="orangered",
                linewidth=4,
                linestyle="--",
                label=f"äºˆæ¸¬ ({years_back}å¹´å‰ã‹ã‚‰)",
                marker="o",
                markersize=8,
                markerfacecolor="white",
                markeredgecolor="orangered",
                markeredgewidth=2,
            )

            # å®Ÿæ¸¬å€¤ã‚’å–å¾—ãƒ»ãƒ—ãƒ­ãƒƒãƒˆ
            train_end_date = (
                pd.to_datetime(result["train_end_date"])
                if isinstance(result["train_end_date"], str)
                else result["train_end_date"]
            )
            forecast_start = train_end_date + pd.DateOffset(months=1)
            actual_values, actual_dates = self.get_actual_values(
                forecast_start, FORECAST_HORIZON
            )
            if len(actual_values) > 0:
                # äºˆæ¸¬æœŸé–“ã¨å®Ÿæ¸¬å€¤æœŸé–“ã‚’åˆã‚ã›ã‚‹
                min_length = min(len(prediction), len(actual_values))
                if min_length > 0:
                    ax.plot(
                        forecast_dates[:min_length],
                        actual_values[:min_length],
                        color="darkgreen",
                        linewidth=3,
                        linestyle="-",
                        label=f"å®Ÿæ¸¬å€¤ ({min_length}ãƒ¶æœˆåˆ†)",
                        marker="s",
                        markersize=6,
                        markerfacecolor="darkgreen",
                        markeredgecolor="white",
                        markeredgewidth=1,
                        alpha=0.9,
                    )

            # ã‚°ãƒ©ãƒ•ã®è£…é£¾
            ax.set_title(
                f"Agentic AI æ™‚ç³»åˆ—äºˆæ¸¬ - {years_back}å¹´å‰ã‹ã‚‰äºˆæ¸¬ (å­¦ç¿’çµ‚äº†: {result['train_end_date']})",
                fontsize=18,
                fontweight="bold",
                pad=25,
            )
            ax.set_xlabel("æ—¥ä»˜", fontsize=14)
            ax.set_ylabel("ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚«ã‚¦ãƒ³ãƒˆ", fontsize=14)
            ax.legend(fontsize=13, loc="upper left")
            ax.grid(True, alpha=0.3, linestyle=":")

            # æ—¥ä»˜è»¸ã®æ›¸å¼è¨­å®š
            ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
            ax.xaxis.set_major_locator(mdates.YearLocator())
            ax.xaxis.set_minor_locator(mdates.MonthLocator(interval=6))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

            # Yè»¸ã®ç¯„å›²èª¿æ•´ï¼ˆå®Ÿæ¸¬å€¤ã‚‚å«ã‚ã‚‹ï¼‰
            all_values = [train_data, prediction]
            if len(actual_values) > 0:
                all_values.append(actual_values)
            combined_values = np.concatenate(all_values)
            y_margin = (np.max(combined_values) - np.min(combined_values)) * 0.1
            ax.set_ylim(
                np.min(combined_values) - y_margin, np.max(combined_values) + y_margin
            )

        plt.tight_layout()

        if save_plots:
            plot_path = self.output_dir / "agentic_ai_forecasting_results.png"
            plt.savefig(
                plot_path,
                dpi=300,
                bbox_inches="tight",
                facecolor="white",
                edgecolor="none",
            )
            print(f"ğŸ“ˆ ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {plot_path}")

        plt.show()

    def save_results(self) -> None:
        """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not self.results:
            print("âŒ ä¿å­˜ã™ã‚‹çµæœãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        print("ğŸ’¾ çµæœä¿å­˜ä¸­...")

        # è©³ç´°çµæœã®ä¿å­˜
        all_results = []
        for years_back, result in self.results.items():
            prediction = result["prediction"]
            forecast_dates = result["forecast_dates"]

            for i, (date, pred_value) in enumerate(
                zip(forecast_dates, prediction, strict=False),
            ):
                all_results.append({
                    "years_back": years_back,
                    "train_end_date": result["train_end_date"],
                    "forecast_date": date,
                    "month_ahead": i + 1,
                    "predicted_value": pred_value,
                })

        results_df = pd.DataFrame(all_results)
        csv_path = self.output_dir / "agentic_ai_forecasting_results.csv"
        results_df.to_csv(csv_path, index=False)
        print(f"ğŸ“ è©³ç´°çµæœ: {csv_path}")

        # ã‚µãƒãƒªãƒ¼çµ±è¨ˆã®ä¿å­˜
        summary_stats = []
        for years_back, result in self.results.items():
            prediction = result["prediction"]
            summary_stats.append({
                "years_back": years_back,
                "train_end_date": result["train_end_date"],
                "prediction_mean": np.mean(prediction),
                "prediction_std": np.std(prediction),
                "prediction_min": np.min(prediction),
                "prediction_max": np.max(prediction),
            })

        summary_df = pd.DataFrame(summary_stats)
        summary_path = self.output_dir / "prediction_summary.csv"
        summary_df.to_csv(summary_path, index=False)
        print(f"ğŸ“Š ã‚µãƒãƒªãƒ¼çµ±è¨ˆ: {summary_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Agentic AI æ™‚ç³»åˆ—äºˆæ¸¬ï¼ˆæ”¹è‰¯ç‰ˆãƒ»ä¿®æ­£æ¸ˆã¿ï¼‰")
    print("=" * 80)

    # è¨­å®š
    control_randomness(seed=RANDOM_SEED)
    data_path = "/home/kimoton/tsfoundations/data/ss/agentic-ai.xlsx"

    # äºˆæ¸¬å™¨ã®åˆæœŸåŒ–ã¨å®Ÿè¡Œ
    forecaster = AgenticAIForecaster(data_path, output_dir="results")

    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆ†æ
        forecaster.load_and_analyze_data()

        # è©•ä¾¡å®Ÿè¡Œ
        forecaster.run_evaluation(max_years_back=3)

        # çµæœã®å¯è¦–åŒ–ã¨ä¿å­˜
        forecaster.visualize_results(save_plots=True)
        forecaster.save_results()

        print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 80)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
